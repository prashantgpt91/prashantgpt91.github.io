---
layout:     post
title:      Intuitive, Interview, Logical Questions
date:       2018-03-17 03:43:57
summary:   
tags: tds
comments: true
---

Questionaire:


 > [Why use softmax only in the output layer and not in hidden layers](https://stackoverflow.com/a/37601915/2058355)  [further explaination](https://stackoverflow.com/questions/44223902/why-is-softmax-not-used-in-hidden-layers)

<!--break-->


 > [Tradeoff batch size vs. number of iterations to train a neural network](https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network)


> [Write your own custom activation function from scratch](https://stackoverflow.com/a/39921608/2058355) [Write your own custom activation function from tensorflow primitives](https://stackoverflow.com/a/45258567/2058355)


> [What do you mean by 1D, 2D and 3D Convolutions in CNN](https://stackoverflow.com/questions/42883547/what-do-you-mean-by-1d-2d-and-3d-convolutions-in-cnn)

> [Common causes of nans during deep training](https://stackoverflow.com/a/33980220/2058355)


> [TensorFlow - introducing both L2 regularization and dropout into the network. Does it Makes sense?](https://stackoverflow.com/q/38292760/2058355)

> [How do CNNs Deal with Position Differences?](https://petewarden.com/2017/10/29/how-do-cnns-deal-with-position-differences/amp/?__twitter_impression=true)

> [How many images do you need to train a neural network?](https://petewarden.com/2017/12/14/how-many-images-do-you-need-to-train-a-neural-network/)

> [what is GEMM in deep learning ?](https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/)

 > [Know more about Gradient descent](http://ruder.io/optimizing-gradient-descent/index.html)

 > [NLP best practices](http://ruder.io/)

 > [Why do we use activation function]

 If we do not apply a Activation function then the output signal would simply be a simple linear function.A linear function is just a polynomial of one degree. Now, a linear equation is easy to solve but they are limited in their complexity and have less power to learn complex functional mappings from data. A Neural Network without Activation function would simply be a Linear regression Model, which has limited power and does not performs good most of the times. We want our Neural Network to not just learn and compute a linear function but something more complicated than that. Also without activation function our Neural network would not be able to learn and model other complicated kinds of data such as images, videos , audio , speech etc. That is why we use Artificial Neural network techniques such as Deep learning to make sense of something complicated ,high dimensional,non-linear -big datasets, where the model has lots and lots of hidden layers in between and has a very complicated architecture which helps us to make sense and extract knowledge form such complicated big datasets. we need to apply a Activation function f(x) so as to make the network more powerfull and add ability to it to learn something complex and complicated form data and represent non-linear complex arbitrary functional mappings between inputs and outputs. Hence using a non linear Activation we are able to generate non-linear mappings from inputs to outputs.

 > [Activation function should be differentiable....Always?](https://stats.stackexchange.com/a/267828)

 > Know internals of Neural network & train efficiently - [1](https://stats.stackexchange.com/a/229015) | [2- Efficient Backprop by Yann Le cun](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf) | [3](https://stats.stackexchange.com/a/250456)

 > [how to take derivative of sigmoid](https://beckernick.github.io/sigmoid-derivative-neural-network/)

 > [The Right Way to Oversample in Predictive Modeling](https://beckernick.github.io/oversampling-modeling/)

 > Implement from scratch - [Neural networks](https://beckernick.github.io/neural-network-scratch/) | [Logistic regression](https://beckernick.github.io/logistic-regression-from-scratch/)

 > [Understanding Convolutions](https://beckernick.github.io/convolutions/)

 > Recommender systems - [1](https://beckernick.github.io/matrix-factorization-recommender/) | [2](https://beckernick.github.io/music_recommender/)

 > NLP - [1](https://beckernick.github.io/law-clustering/) | [2](https://beckernick.github.io/yelp-reviews/)

 > Scrapping - [1](https://beckernick.github.io/job-discovery/) | [2]

 > Data Engineering - [1](https://beckernick.github.io/mapreduce-python-hive/) |

 > [Record Linkage](https://beckernick.github.io/parallel-record-linkage/)

 > [Knapsack](https://beckernick.github.io/dynamic-programming-knapsack/)

 > [Fitbit activity prediction](https://beckernick.github.io/activity-prediction/)

 > [Vanishing gradient problem](https://medium.com/@anishsingh20/the-vanishing-gradient-problem-48ae7f501257)

 > [Walkthrough of back-propagation](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/) | [Derivation of back-propagation](https://sefiks.com/2017/01/21/the-math-behind-backpropagation/)