---
layout:     post
title:      Machine Learning Interview | Basics | Quick revision
categories: blog   
tags: theoritical data science
---

Here it goes


1. [Deep learning Error Analysis (Bias variance, train/train-dev/dev/test)](https://docs.google.com/document/d/1ISYM3djT2jzfCPDXuwU1goFF_DNSinuWdh6spjc3Dd0/edit)
<!--break-->

2. [Why use softmax only in the output layer and not in hidden layers](https://stackoverflow.com/a/37601915/2058355)  [further explaination](https://stackoverflow.com/questions/44223902/why-is-softmax-not-used-in-hidden-layers)


3. [Tradeoff batch size vs. number of iterations to train a neural network](https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network)


4. [Write your own custom activation function from scratch](https://stackoverflow.com/a/39921608/2058355) [Write your own custom activation function from tensorflow primitives](https://stackoverflow.com/a/45258567/2058355)


5. [What do you mean by 1D, 2D and 3D Convolutions in CNN](https://stackoverflow.com/questions/42883547/what-do-you-mean-by-1d-2d-and-3d-convolutions-in-cnn)

6. [Common causes of nans during deep training](https://stackoverflow.com/a/33980220/2058355)


7. [Introducing both L2 regularization and dropout into the network. Does it Makes sense?](https://stackoverflow.com/q/38292760/2058355)

8. [How do CNNs Deal with Position Differences?](https://petewarden.com/2017/10/29/how-do-cnns-deal-with-position-differences/amp/?__twitter_impression=true)

9. [How many images do you need to train a neural network?](https://petewarden.com/2017/12/14/how-many-images-do-you-need-to-train-a-neural-network/)

10. [what is GEMM in deep learning ?](https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/)

11. [Know more about Gradient descent](http://ruder.io/optimizing-gradient-descent/index.html)

12. [NLP best practices](http://ruder.io/)

13. [Activation function should be differentiable..Always?](https://stats.stackexchange.com/a/267828)

14. Know internals of Neural network & train efficiently - [1](https://stats.stackexchange.com/a/229015)

15. [Efficient Backprop by Yann Le cun](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)

16. [tanh activation function vs sigmoid activation function](https://stats.stackexchange.com/a/250456)

17. [how to take derivative of sigmoid](https://beckernick.github.io/sigmoid-derivative-neural-network/)

18. [The Right Way to Oversample in Predictive Modeling](https://beckernick.github.io/oversampling-modeling/)

19. [Implement from scratch - Neural networks](https://beckernick.github.io/neural-network-scratch/)

20. [Implement from scratch - Logistic regression](https://beckernick.github.io/logistic-regression-from-scratch/)

21. [Understanding Convolutions](https://beckernick.github.io/convolutions/)

22. [Vanishing gradient problem](https://medium.com/@anishsingh20/the-vanishing-gradient-problem-48ae7f501257)

23. [Walkthrough of back-propagation](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)

24. [Derivation of back-propagation](https://sefiks.com/2017/01/21/the-math-behind-backpropagation/)

25. Why do we use activation function ?

 <script src="https://gist.github.com/x0v/a87822803b679fd7c5b985161d3212ef.js"></script>


